{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d02dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main packages\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755a8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/11 18:52:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark context\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setAppName(\"wiki\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004eadeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             article|            category|\n",
      "+--------------------+--------------------+\n",
      "|%C3%81ed%C3%A1n_m...|subject.History.B...|\n",
      "|%C3%81ed%C3%A1n_m...|subject.People.Hi...|\n",
      "|          %C3%85land|   subject.Countries|\n",
      "|          %C3%85land|subject.Geography...|\n",
      "|  %C3%89douard_Manet|subject.People.Ar...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read dataframe categories.tsv\n",
    "df_cat = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(\"hdfs://localhost:9000/user/bigdata2022/datasets/wiki/categories.tsv\")\n",
    "df_cat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003048c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unquote article and category\n",
    "from urllib.parse import unquote\n",
    "rdd_cat = df_cat.rdd\n",
    "rdd_cat  = rdd_cat.map(lambda x: (unquote(x.article),unquote(x.category)))\n",
    "#rdd_cat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5534b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the categories per article in an array \n",
    "rdd_cat = rdd_cat.map(lambda x: (x[0],[x[1]])) \\\n",
    "            .reduceByKey(lambda a, b: a+b)\n",
    "#rdd_cat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa294b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              source|   destination|\n",
      "+--------------------+--------------+\n",
      "|%C3%81ed%C3%A1n_m...|          Bede|\n",
      "|%C3%81ed%C3%A1n_m...|       Columba|\n",
      "|%C3%81ed%C3%A1n_m...|D%C3%A1l_Riata|\n",
      "|%C3%81ed%C3%A1n_m...| Great_Britain|\n",
      "|%C3%81ed%C3%A1n_m...|       Ireland|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read dataframe links.tsv\n",
    "df_link = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(\"hdfs://localhost:9000/user/bigdata2022/datasets/wiki/links.tsv\")\n",
    "df_link.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c013f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unquote source and destination article\n",
    "from urllib.parse import unquote\n",
    "rdd_link = df_link.rdd\n",
    "rdd_link  = rdd_link.map(lambda x: (unquote(x.source),unquote(x.destination)))\n",
    "#rdd_link.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69254f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------+--------------------+------+\n",
      "| hashedIpAddress| timestamp|durationInSec|                path|rating|\n",
      "+----------------+----------+-------------+--------------------+------+\n",
      "|6a3701d319fc3754|1297740409|          166|14th_century;15th...|  NULL|\n",
      "|3824310e536af032|1344753412|           88|14th_century;Euro...|     3|\n",
      "|415612e93584d30e|1349298640|          138|14th_century;Nige...|  NULL|\n",
      "|64dd5cd342e3780c|1265613925|           37|14th_century;Rena...|  NULL|\n",
      "|015245d773376aab|1366730828|          175|14th_century;Ital...|     3|\n",
      "+----------------+----------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read dataframe path_finished.tsv\n",
    "df_path = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(\"hdfs://localhost:9000/user/bigdata2022/datasets/wiki/paths_finished.tsv\")\n",
    "df_path.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ad5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructed a new rdd with the source and destination of the path, \n",
    "# the duration in seconds of the path, the rating of the path, \n",
    "# a variable (worth 0 if the rating is unknown or worth 1 if it is known) \n",
    "# and the length of the path.\n",
    "def formatRow(x):\n",
    "    values = unquote(x.path).split(\";\")\n",
    "    length = len(values)\n",
    "    if(x.rating == \"NULL\"):\n",
    "        rating = 0\n",
    "        ratingNum = 0\n",
    "    else:\n",
    "        rating = int(x.rating)\n",
    "        ratingNum = 1\n",
    "    return (values[0]+\"::\"+values[-1],int(x.durationInSec),rating,ratingNum,length)\n",
    "\n",
    "rdd_path = df_path.rdd\n",
    "rdd_path  = rdd_path.map(lambda x: formatRow(x))\n",
    "#rdd_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22ed9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summed up the values of the rating variable for each distinct path\n",
    "rdd_rating  = rdd_path.map(lambda x: (x[0],x[2])) \\\n",
    "                .reduceByKey(lambda a, b: a+b)\n",
    "#rdd_rating.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7757ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summed up the number of user rating for each separate path\n",
    "rdd_ratingsNum  = rdd_path.map(lambda x: (x[0],x[3])) \\\n",
    "                    .reduceByKey(lambda a, b: a+b)\n",
    "#rdd_ratingsNum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658d8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rating of each individual path\n",
    "def divide(x,y):\n",
    "    return round(x/y) if y else 0 \n",
    "    \n",
    "rdd_rating2 = rdd_rating.union(rdd_ratingsNum).reduceByKey(lambda x,y : divide(x,y))\n",
    "#rdd_rating2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dccbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average duration in seconds of a single path\n",
    "import math\n",
    "\n",
    "rdd_duration = rdd_path \\\n",
    "    .map(lambda x: (x[0], x[1])) \\\n",
    "    .mapValues(lambda v: (v, 1)) \\\n",
    "    .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "    .mapValues(lambda v: math.ceil(v[0]/v[1]))\n",
    "#rdd_duration.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255ffffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average length of each individual path\n",
    "rdd_length = rdd_path \\\n",
    "    .map(lambda x: (x[0], x[4])) \\\n",
    "    .mapValues(lambda v: (v, 1)) \\\n",
    "    .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "    .mapValues(lambda v: math.ceil(v[0]/v[1]))\n",
    "#rdd_length.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00087786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD containing the path, average rating and average duration in seconds  \n",
    "rdd_final1 = rdd_rating2.join(rdd_duration)\n",
    "#rdd_final1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b059ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD containing the path, average duration in seconds, average length and average rating\n",
    "rdd_final2 = rdd_final1.join(rdd_length).map(lambda x: (x[0],x[1][0][1],x[1][1],x[1][0][0]))\n",
    "#rdd_final2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc23842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD containing the source path, the destination path, \n",
    "# average duration in seconds, average length and average rating\n",
    "def splitRow(x):\n",
    "    source, target = x[0].split(\"::\")\n",
    "    return (source, target, x[1], x[2], x[3])\n",
    "\n",
    "rdd_final3  = rdd_final2.map(lambda x: splitRow(x))\n",
    "#rdd_final3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f7cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert RDD to DF\n",
    "\n",
    "df_path = rdd_final3.toDF([\"source\",\"target\",\"duration\",\"pathLength\",\"rating\"])\n",
    "#df_path.printSchema()\n",
    "#df_path.show(truncate=False)\n",
    "\n",
    "df_cat = rdd_cat.toDF([\"source\",\"category\"])\n",
    "#df_cat.printSchema()\n",
    "#df_cat.show(truncate=False)\n",
    "\n",
    "df_link = rdd_link.toDF([\"source\",\"target\"])\n",
    "#df_link.printSchema()\n",
    "#df_link.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7bc04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_path.write.option(\"header\",True) \\\n",
    "# .csv(\"hdfs://localhost:9000/user/bigdata2022/output/wiki/paths.csv\")\n",
    "\n",
    "#df_cat.write.option(\"header\",True) \\\n",
    "# .csv(\"hdfs://localhost:9000/user/bigdata2022/output/wiki/categories.csv\")\n",
    "\n",
    "#df_link.write.option(\"header\",True) \\\n",
    "# .csv(\"hdfs://localhost:9000/user/bigdata2022/output/wiki/links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f036b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserted article nodes, with their respective categories, into the neo4j database\n",
    "from pyspark.sql.functions import col\n",
    "from py2neo import Graph\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"bigdata\"))\n",
    "\n",
    "source = df_cat.select('source').rdd.flatMap(lambda x: x).collect()\n",
    "category = df_cat.select('category').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "length = len(source)\n",
    "\n",
    "for i in range(length):\n",
    "    #print(str(i) +\" on \"+str(length))\n",
    "    graph.run('CREATE (n:Article {name: \"'+source[i]+'\", category: \"'+str(category[i])+'\"})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f26cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inserted direct edges between two different articles (hyperlink) into the Neo4j database\n",
    "source = df_link.select('source').rdd.flatMap(lambda x: x).collect()\n",
    "target = df_link.select('target').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "length = len(source)\n",
    "\n",
    "for i in range(length):\n",
    "    #print(str(i) +\" on \"+str(length))\n",
    "    graph.run('MATCH (a:Article), (b:Article) WHERE a.name = \"'+source[i]+'\" AND b.name = \"'+target[i]+'\"'+\n",
    "              'CREATE (a)-[r:HYPERLINK]->(b) RETURN type(r)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f7ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inserted direct edges between two different articles of a path (challenge), \n",
    "# with the respective average rating, average duration in seconds and \n",
    "# average length in seconds, into the Neo4j database\n",
    "source = df_path.select('source').rdd.flatMap(lambda x: x).collect()\n",
    "target = df_path.select('target').rdd.flatMap(lambda x: x).collect()\n",
    "duration = df_path.select('duration').rdd.flatMap(lambda x: x).collect()\n",
    "pathLength = df_path.select('pathLength').rdd.flatMap(lambda x: x).collect()\n",
    "rating = df_path.select('rating').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "length = len(source)\n",
    "\n",
    "for i in range(length):\n",
    "    #print(str(i) +\" on \"+str(length))\n",
    "    graph.run('MATCH (a:Article), (b:Article) WHERE a.name = \"'+source[i]+'\" AND b.name = \"'+target[i]+'\"'+\n",
    "              'CREATE (a)-[r:CHALLENGE]->(b) '+\n",
    "              'SET r.duration = \"'+str(duration[i])+'\", '+\n",
    "              'r.pathLength = \"'+str(pathLength[i])+'\", '+\n",
    "              'r.rating = \"'+str(rating[i])+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11ea5d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
